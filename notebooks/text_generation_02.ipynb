{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Text Generation \n",
    "> A Practioners Guide : Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature is a scaling factor applied to the outputs of our dense layer before applying the softmaxactivation function. In a nutshell, it defines how conservative or \"creative\" the model's guesses are for the next character in a sequence. Lower values of temperature (e.g., 0.2) will generate \"safe\" guesses whereas values of temperature above 1.0 will start to generate \"riskier\" guesses. Think of it as the amount of surpise you'd have at seeing an English word start with \"st\" versus \"sg\". When temperature is low, we may get lots of \"the\"s and \"and\"s; when temperature is high, things get more unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Generator from Scratch\n",
    "\n",
    "<img src=\"illustrations/tf_logo.png\" >\n",
    "\n",
    "We discussed about **RNNs** and **language models** in the previous notebook. Lets get our hands dirty and train our very own language model from scratch.\n",
    "\n",
    "We will train a language model using Tensorflow 2.0. TF2.0 is the updated version of the already popular deep learning framework. TF2.0 provides keras based high level APIs along with core set of functionality along with eager execution for more complex workflows. We will be relying this session using TF+Keras setup which is easy to understand and deploy.\n",
    "\n",
    "\n",
    "This notebook will leverage **GRUs** inplace of vanilla RNNs for two main reasons, better at handling vanishing and exploding gradients as well as ability to handle longer context. As far as corpus for training our language model, we utilize the famous book _**The Adventures of Sherlock Holmes**_ by _Sir Arthur Conan Doyle_. The book is made available through _Project Gutenberg_, check references section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version=2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version={}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = r'data/the_adventures_of_sherlock_holmes_1661-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book contains a total of 594197 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "text = open(datafile_path, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Book contains a total of {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick snippet of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. A SCANDAL IN BOHEMIA\r\n",
      "\r\n",
      "\r\n",
      "I.\r\n",
      "\r\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\r\n",
      "mention her under any other name. In his eyes she eclipses and\r\n",
      "predominates the whole of her \n"
     ]
    }
   ],
   "source": [
    "print(text[1300:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Text\n",
    "\n",
    "We shall perform bare minimum clean up of the text. The aim is to help our model understand the usage of words and its context. Typical preprocessing steps such as stopword removal, stemming, lower casing etc. are not required in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove first 1300 characters to remove \n",
    "# details related to project gutenberg\n",
    "text = text [1300:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Character Count | Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character to Integer Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '\"' :   4,\n",
      "  '$' :   5,\n",
      "  '%' :   6,\n",
      "  '&' :   7,\n",
      "  \"'\" :   8,\n",
      "  '(' :   9,\n",
      "  ')' :  10,\n",
      "  '*' :  11,\n",
      "  ',' :  12,\n",
      "  '-' :  13,\n",
      "  '.' :  14,\n",
      "  '/' :  15,\n",
      "  '0' :  16,\n",
      "  '1' :  17,\n",
      "  '2' :  18,\n",
      "  '3' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Integer Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sherlock Holmes, by ' ---- char-2-int ----  [61 74 68 71 59 67  2 37 71 68 69 61 75  2 75 64 61  2 65 75]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- char-2-int ----  {}'.format(repr(text[40:60]), text_as_int[40:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We leverage a sliding window approach to train out model. We first set the maximum sequence length to 100 characters. This is done for the purposes of preparing and training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      ".\n",
      " \n",
      "A\n",
      " \n",
      "S\n",
      "C\n",
      "A\n",
      "N\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard '\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'him\\r\\nmention her under any other name. In his eyes she eclipses and\\r\\npredominates the whole of her se'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'x. It was not that he felt any emotion\\r\\nakin to love for Irene Adler. All emotions, and that one part'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'icularly,\\r\\nwere abhorrent to his cold, precise but admirably balanced mind. He\\r\\nwas, I take it, the m'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'ost perfect reasoning and observing machine that\\r\\nthe world has seen, but as a lover he would have pl'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'aced himself in a\\r\\nfalse position. He never spoke of the softer passions, save with a gibe\\r\\nand a sne'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'er. They were admirable things for the observer—excellent for\\r\\ndrawing the veil from men’s motives an'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'d actions. But for the trained\\r\\nreasoner to admit such intrusions into his own delicate and finely\\r\\na'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'djusted temperament was to introduce a distracting factor which might\\r\\nthrow a doubt upon all his men'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'tal results. Grit in a sensitive\\r\\ninstrument, or a crack in one of his own high-power lenses, would n'\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(10):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    Utility which takes a chunk of input text and target as one position shifted form of input chunk.\n",
    "    Parameters:\n",
    "        chunk: input list of words\n",
    "    Returns:\n",
    "        Tuple-> input_text(i.e. chunk minus last word),target_text(input chunk minus the first word)\n",
    "    \"\"\"\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'I. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard'\n",
      "Target data: '. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 38 ('I')\n",
      "  expected output: 14 ('.')\n",
      "Step    1\n",
      "  input: 14 ('.')\n",
      "  expected output: 2 (' ')\n",
      "Step    2\n",
      "  input: 2 (' ')\n",
      "  expected output: 30 ('A')\n",
      "Step    3\n",
      "  input: 30 ('A')\n",
      "  expected output: 2 (' ')\n",
      "Step    4\n",
      "  input: 2 (' ')\n",
      "  expected output: 48 ('S')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape=<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"Dataset Shape={}\".format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "\n",
    "We prepare a utility function to generate the architecture of our deep learning based language model. We leverage the high level ```tf.keras``` API for creating this model. We use only 1 hidden layer. You may experiment with additional layers as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \"\"\"\n",
    "    Utility to create a model object.\n",
    "    Parameters:\n",
    "        vocab_size: number of unique characters\n",
    "        embedding_dim: size of embedding vector. This typically in powers of 2, i.e. 64, 128, 256 and so on\n",
    "        rnn_units: number of GRU units to be used\n",
    "        batch_size: batch size for training the model\n",
    "    Returns:\n",
    "        tf.keras model object\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           24576     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 96)            98400     \n",
      "=================================================================\n",
      "Total params: 4,061,280\n",
      "Trainable params: 4,061,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Callbacks\n",
    "- We setup a single callback to store training checkpoints. \n",
    "- You may leverage other callbacks such as tensorboard, earlystopping etc as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = r'data/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Train the ~Dragon🐉~ Language Model \n",
    "\n",
    "Now that we have prepared our training dataset along with our model, let us train it. We train it for a few epochs and observe the loss to understand whether it is learning or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "91/91 [==============================] - 337s 4s/step - loss: 2.9753\n",
      "Epoch 2/12\n",
      "91/91 [==============================] - 381s 4s/step - loss: 2.2011\n",
      "Epoch 3/12\n",
      "91/91 [==============================] - 498s 5s/step - loss: 1.9793\n",
      "Epoch 4/12\n",
      "91/91 [==============================] - 441s 5s/step - loss: 1.7988\n",
      "Epoch 5/12\n",
      "91/91 [==============================] - 437s 5s/step - loss: 1.6527\n",
      "Epoch 6/12\n",
      "91/91 [==============================] - 387s 4s/step - loss: 1.5329\n",
      "Epoch 7/12\n",
      "91/91 [==============================] - 413s 5s/step - loss: 1.4404\n",
      "Epoch 8/12\n",
      "91/91 [==============================] - 400s 4s/step - loss: 1.3671\n",
      "Epoch 9/12\n",
      "91/91 [==============================] - 393s 4s/step - loss: 1.3109\n",
      "Epoch 10/12\n",
      "91/91 [==============================] - 392s 4s/step - loss: 1.2607\n",
      "Epoch 11/12\n",
      "91/91 [==============================] - 394s 4s/step - loss: 1.2185\n",
      "Epoch 12/12\n",
      "91/91 [==============================] - 395s 4s/step - loss: 1.1772\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Text\n",
    "\n",
    "We trained out model on the text from _The Adventures of Sherlock Holmes_. Now one should notice that we literally did not perform any preprocessing on the text apart from removing some metadata and table of contents. The model is trained with a vocab size of **96** unique characters which includes numbers and special characters apart from lower and upper case letters.\n",
    "\n",
    "We should also note that we have trained a character level language model to reduce the vocab size. Imagine the vocab size for training at the word level, wouldn't it be orders of magnitude larger than this? Also imagine the amount of training data required to help the model understand different contexts under which a specific word might be used.\n",
    "\n",
    "Let us generate some text and see what our model has learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/training_checkpoints/ckpt_12'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the latest checkpoint from the model directory\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load\n",
    "> Notice that we trained the model with certain batch size. Using ```model.summary``` we saw how the batch size shows up as one the parameters which determine input's shape.\n",
    "\n",
    "> For inference, we would be using a single input sentence/context to generate text. Thus we build the model again using ```build_model``` utility we prepared earlier but use a ```batch_size``` of 1 this time. Once we have the model object with desired batch size, we use ```load_weights``` to utilize the latest checkpoint weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            24576     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 96)             98400     \n",
      "=================================================================\n",
      "Total params: 4,061,280\n",
      "Trainable params: 4,061,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, context_string, num_generate=1000,temperature=1.0):\n",
    "    \"\"\"\n",
    "    Utility to generate text given a trained model and context\n",
    "    Parameters:\n",
    "        model: tf.keras object trained on a sufficiently sized corpus\n",
    "        context_string: input string which acts as context for the model\n",
    "        num_generate: number of characters to be generated\n",
    "        temperature: parameter to control randomness of outputs\n",
    "    Returns:\n",
    "        string : context_string+text_generated\n",
    "    \"\"\"\n",
    "\n",
    "    # vectorizing: convert context string into string indices\n",
    "    input_eval = [char2idx[s] for s in context_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # String for generated characters\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    # Loop till required number of characters are generated\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # temperature helps control the character returned by the model.\n",
    "        predictions = predictions / temperature\n",
    "        # Sampling over a categorical distribution\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # predicted character acts as input for next step\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (context_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us generate some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson you are certainly leave\r\n",
      "your husband’s read, and then two vareak illsted of bory, it was young McCarthy’t \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, context_string=u\"Watson you are\",num_generate=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson you are that prsilU\r",
      "S4œlder, he\r\n",
      "int always,\r\n",
      "heabveirnies brokeve at Togheatudeprégail I; ‘Ych—_ OR2Uà,D“W\n"
     ]
    }
   ],
   "source": [
    "# We increase the temperature, i.e. increase randomness\n",
    "print(generate_text(model, context_string=u\"Watson you are\",num_generate=100,temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson you are the news\r\n",
      "all open. It was a man who was a man with a sudden brightly more than I cannot means to w\n"
     ]
    }
   ],
   "source": [
    "# We decrease the temperature, i.e. increase randomness\n",
    "print(generate_text(model, context_string=u\"Watson you are\",num_generate=100,temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ Project Gutenberg : [The Adventures of Sherlock Holmes](https://www.gutenberg.org/ebooks/1661)\n",
    "+ [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "+ [Freecode](https://www.freecodecamp.org/news/applied-introduction-to-lstms-for-text-generation-380158b29fb3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
